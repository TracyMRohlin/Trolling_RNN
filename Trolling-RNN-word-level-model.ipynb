{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Download annotated comments and annotations. \n",
    "# If you're Tracy, Courtney, or Amandalynne, don't run this step \n",
    "# because you already have the data! If you aren't us, you will \n",
    "# probably need to do this step. \n",
    "# It will take a while. \n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7038044' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7383751' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SentenceGenerator():\n",
    "        def __init__(self, df):\n",
    "            self.df = df\n",
    "            self.sentences = []\n",
    "            self.vocab = set()\n",
    "        def gen_sentences(self):\n",
    "            for sentence in self.df['comment']:\n",
    "                tokens = word_tokenize(sentence)\n",
    "                self.sentences.append(tokens)\n",
    "                self.vocab.update(tokens)\n",
    "            return self.sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the data into a Pandas dataframe.\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')\n",
    "\n",
    "# Label a comment as an attack if over half of annotators did so.\n",
    "# We can tinker with this threshold later.\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5\n",
    "\n",
    "# Join labels and comments\n",
    "comments['attack'] = labels\n",
    "\n",
    "# Preprocess the data -- remove newlines, tabs, quotes\n",
    "# Something to consider: remove Wikipedia style markup (::'s and =='s)\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"`\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Grab the training data (seems to be 60%)\n",
    "train_data = comments.loc[comments['split'] == 'train']\n",
    "valid_data = comments.loc[comments['split'] == 'dev']\n",
    "test_data = comments.loc[comments['split'] == 'test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    }
   ],
   "source": [
    "train = SentenceGenerator(train_data)\n",
    "train_sentences = train.gen_sentences()[:11000]\n",
    "\n",
    "valid = SentenceGenerator(valid_data)\n",
    "valid_sentences = valid.gen_sentences()[:3000]\n",
    "\n",
    "test = SentenceGenerator(test_data)\n",
    "test_sentences = test.gen_sentences()[:3000]\n",
    "print(len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# max number of words found in all docs\n",
    "#vocab_size = len(train.vocab)\n",
    "vocab_size = 5000\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wordvec_model = Word2Vec(sentences=train_sentences, size=200, sg=1, max_vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sentences(sentences, w2v_model, vocab_size):\n",
    "    num_sentences = len(sentences) *0.5\n",
    "    X = np.zeros((num_sentences, vocab_size, 200), dtype=np.float32)\n",
    "    \n",
    "    empty_word = np.zeros((200), dtype=np.float32)\n",
    "    for idx, s in enumerate(sentences):\n",
    "        if idx == num_sentences:\n",
    "            break\n",
    "        for jdx, word in enumerate(s):\n",
    "            if jdx == vocab_size:\n",
    "                break \n",
    "            else:\n",
    "                if word in w2v_model:\n",
    "                    X[idx, jdx, :] = w2v_model[word]\n",
    "                else:\n",
    "                    X[idx, jdx, :] = empty_word\n",
    "    yield X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000,) (3000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data[\"attack\"]\n",
    "train_labels = train_labels.iloc[:11000]\n",
    "valid_labels = valid_data[\"attack\"]\n",
    "valid_labels = valid_labels.iloc[:3000]\n",
    "test_labels = test_data[\"attack\"]\n",
    "test_labels = test_labels.iloc[:3000]\n",
    "print(train_labels.shape, valid_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X = list(vectorize_sentences(train_sentences, wordvec_model, vocab_size))\n",
    "valid_X = list(vectorize_sentences(valid_sentences, wordvec_model, vocab_size))\n",
    "test_X = list(vectorize_sentences(test_sentences, wordvec_model, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2641d641cc91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(train_X.shape, valid_X.shape, test_X.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# emulating the model by having 200 \"hashes\" with 50 layers\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Dense(200, input_shape=(vocab_size, 200)))\n",
    "model.add(Flatten())\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected flatten_11 to have shape (None, 11161200) but got array with shape (11238, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-ea3da739bb23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# don't know if this works yet because patas is down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#score, acc = model.evaluate(X_test, Y_test, batch_size=128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1407\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                                     exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1301\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1302\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    131\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected flatten_11 to have shape (None, 11161200) but got array with shape (11238, 1)"
     ]
    }
   ],
   "source": [
    "# don't know if this works yet because patas is down.\n",
    "# Train model\n",
    "model.fit(train_X, train_labels, batch_size=128, nb_epoch=4)\n",
    "# Evaluate model\n",
    "#score, acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
